# -*- coding: utf-8 -*-
"""ê¸‰ì‹ ë°ì´í„° ë¶„ì„.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cvVc1qMQ8OBFz5Mw67kuZSLx9wEPqSYi
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer


# SentenceTransformerë¥¼ sentence_transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from sentence_transformers import SentenceTransformer
# KMeansë¥¼ sklearn.clusterì—ì„œ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from sklearn.cluster import KMeans
# numpyë¥¼ npë¡œ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
import numpy as np

import pandas as pd
import os

%%writefile app.py
file_path = '/content/drive/MyDrive/á„€á…³á†¸á„‰á…µá†¨ á„‰á…¥á†¯á„†á…®á†«á„Œá…©á„‰á…¡ á„Œá…¥á†«á„á…¦á„€á…µá„€á…¡á†«.csv'

df = pd.read_csv(file_path, encoding='utf-8')

# ë°ì´í„° í™•ì¸
print(df.head())

# ê°ê´€ì‹
if 'í•™ë…„' in df.columns:
    print(df['í•™ë…„'].value_counts())
if 'ì´ë²ˆì£¼ ë§Œì¡±ë„' in df.columns:
    print(df['ì´ë²ˆì£¼ ë§Œì¡±ë„'].value_counts())
if 'ì´ë²ˆì£¼ ê°€ì¥ ì¢‹ì•˜ë˜ ê¸‰ì‹' in df.columns:
    print(df['ì´ë²ˆì£¼ ê°€ì¥ ì¢‹ì•˜ë˜ ê¸‰ì‹'].value_counts())
if 'ì´ë²ˆì£¼ ê¸‰ì‹ì´ ì¢‹ì•˜ë˜ ì´ìœ ' in df.columns:
    print(df['ì´ë²ˆì£¼ ê¸‰ì‹ì´ ì¢‹ì•˜ë˜ ì´ìœ '].value_counts())
if 'ì´ë²ˆì£¼ ê°€ì¥ ì‹«ì—ˆë˜ ê¸‰ì‹' in df.columns:
    print(df['ì´ë²ˆì£¼ ê°€ì¥ ì‹«ì—ˆë˜ ê¸‰ì‹'].value_counts())
if 'ì”ë°˜ ë¹„ìœ¨' in df.columns:
    print(df['ì”ë°˜ ë¹„ìœ¨'].value_counts())
if 'ìˆ˜ë©´ì‹œê°„' in df.columns:
    print(df['ìˆ˜ë©´ì‹œê°„'].value_counts())

# ê°ê´€ì‹ í•­ëª© ë¦¬ìŠ¤íŠ¸
objective_columns = ['í•™ë…„', 'ì´ë²ˆì£¼ ë§Œì¡±ë„', 'ì´ë²ˆì£¼ ê°€ì¥ ì¢‹ì•˜ë˜ ê¸‰ì‹', 'ì”ë°˜ ë¹„ìœ¨', 'ìˆ˜ë©´ì‹œê°„']

for col in objective_columns:
    if col in df.columns:
        plt.figure(figsize=(8, 5))
        sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette='Set2')
        plt.title(f'{col} ì‘ë‹µ ë¶„í¬')
        plt.xlabel(col)
        plt.ylabel('ì‘ë‹µ ìˆ˜')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

for col in ['ì´ë²ˆì£¼ ë§Œì¡±ë„', 'ì”ë°˜ ë¹„ìœ¨']:
    if col in df.columns:
        plt.figure(figsize=(6, 6))
        df[col].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, counterclock=False)
        plt.title(f'{col} ë¹„ìœ¨')
        plt.ylabel('')
        plt.show()

# ì„œìˆ í˜•

# ë¬¸ì¥ ì»¬ëŸ¼ ì§€ì • (ì˜ˆ: 'ê±´ì˜ì‚¬í•­' ì»¬ëŸ¼)
text_column = 'ì¶”ê°€ ë©”ë‰´ì™€ ê±´ì˜ì‚¬í•­'

# ë¬¸ì¥ ë¶„í•  í•¨ìˆ˜
def split_sentences(text):
    text = re.sub(r'[.?!]', '', text)  # ë§ˆì¹¨í‘œ ë“± ì œê±°
    return re.split(r',|ê·¸ë¦¬ê³ |ë˜ëŠ”|ë°|&|/|ë˜\s+|ê·¸ë¦¬ê³ \s+', text)

# ë¶„ì ˆëœ ë¬¸ì¥, ì›ë³¸ë¬¸ì¥ ì¸ë±ìŠ¤, ì›ë³¸ë¬¸ì¥ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
split_texts = []
original_indices = []
original_sentences = []

for idx, text in df[text_column].dropna().astype(str).items():
    splits = split_sentences(text)
    for part in splits:
        cleaned = part.strip()
        if cleaned:
            split_texts.append(cleaned)
            original_indices.append(idx)
            original_sentences.append(text)

# ì„ë² ë”©
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
embeddings = model.encode(split_texts)

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

"""embeddings = model.encode(texts)"""

sse = []
k_range = range(2, 30)  # 2~14 êµ°ì§‘ ìˆ˜ ì‹œë„
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(embeddings)
    sse.append(kmeans.inertia_)  # SSE ê°’ ì €ì¥

plt.plot(k_range, sse, 'bx-')
plt.xlabel('êµ°ì§‘ ìˆ˜ (k)')
plt.ylabel('SSE (ì˜¤ì°¨ ì œê³±í•©)')
plt.title('ì—˜ë³´ìš° ê¸°ë²•ìœ¼ë¡œ ìµœì  êµ°ì§‘ ìˆ˜ ì°¾ê¸°')
plt.show()

# êµ°ì§‘ ìˆ˜ ì„¤ì •
n_clusters = 8
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
labels = kmeans.fit_predict(embeddings)

# ê²°ê³¼ DataFrame ìƒì„±
result_df = pd.DataFrame({
    'ë¬¸ì¥_ë¶„ì ˆ': split_texts,
    'êµ°ì§‘': labels,
    'ì›ë³¸ë¬¸ì¥_index': original_indices,
    'ì›ë³¸ë¬¸ì¥': original_sentences
})

# ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ set ì‚¬ìš©
"""print(f'\n=== [{text_column}] êµ°ì§‘í™” ê²°ê³¼ (ì›ë³¸ ë¬¸ì¥ ê¸°ì¤€) ===')
for i in range(n_clusters):
    cluster_data = result_df[result_df['êµ°ì§‘'] == i]
    unique_originals = cluster_data[['ì›ë³¸ë¬¸ì¥']].drop_duplicates().reset_index(drop=True)

    print(f'\n[êµ°ì§‘ {i}] ì›ë³¸ë¬¸ì¥ ëª©ë¡ (ì´ {len(unique_originals)}ê±´):')
    for j, row in unique_originals.iterrows():
        print(f'- {row["ì›ë³¸ë¬¸ì¥"]}')"""

print(f'\n=== [{text_column}] êµ°ì§‘í™” ê²°ê³¼ (êµ°ì§‘ ì´ë¦„ í¬í•¨) ===')
cluster_names = {}
for i in range(n_clusters):
    cluster_data = result_df[result_df['êµ°ì§‘'] == i]
    cluster_sentences = cluster_data['ë¬¸ì¥_ë¶„ì ˆ'].tolist()

    # TF-IDFë¡œ ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ
    vectorizer = TfidfVectorizer(max_features=20, stop_words='english')  # í•„ìš”ì‹œ í•œêµ­ì–´ ë¶ˆìš©ì–´ ì²˜ë¦¬ ê°€ëŠ¥
    tfidf_matrix = vectorizer.fit_transform(cluster_sentences)
    feature_names = vectorizer.get_feature_names_out()
    scores = tfidf_matrix.sum(axis=0).A1
    top_idx = scores.argmax()
    cluster_keyword = feature_names[top_idx]

    cluster_names[i] = cluster_keyword

    # ì›ë³¸ë¬¸ì¥ ëª©ë¡ ì¶œë ¥
    unique_originals = cluster_data[['ì›ë³¸ë¬¸ì¥']].drop_duplicates().reset_index(drop=True)
    print(f'\n[êµ°ì§‘ {i} - "{cluster_keyword}"] ì›ë³¸ë¬¸ì¥ ëª©ë¡ (ì´ {len(unique_originals)}ê±´):')
    for j, row in unique_originals.iterrows():
        print(f'- {row["ì›ë³¸ë¬¸ì¥"]}')

import matplotlib.pyplot as plt
import seaborn as sns

# êµ°ì§‘ í‚¤ì›Œë“œ ì´ë¦„ ë¶™ì´ê¸°
result_df['êµ°ì§‘ëª…'] = result_df['êµ°ì§‘'].map(cluster_names)

# êµ°ì§‘ë³„ ê°œìˆ˜ ì„¸ê¸°
cluster_counts = result_df['êµ°ì§‘ëª…'].value_counts().sort_values(ascending=False)

plt.figure(figsize=(10, 5))
sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='pastel')
plt.title('ì„œìˆ í˜• ì‘ë‹µ êµ°ì§‘ë³„ ë¬¸ì¥ ìˆ˜')
plt.xlabel('êµ°ì§‘ í‚¤ì›Œë“œ')
plt.ylabel('ë¬¸ì¥ ìˆ˜')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

from sklearn.decomposition import PCA

# 2D ì„ë² ë”©ìœ¼ë¡œ ì¶•ì†Œ
pca = PCA(n_components=2)
reduced = pca.fit_transform(embeddings)

reduced_df = pd.DataFrame({
    'x': reduced[:, 0],
    'y': reduced[:, 1],
    'êµ°ì§‘': result_df['êµ°ì§‘'],
    'êµ°ì§‘ëª…': result_df['êµ°ì§‘ëª…']
})

plt.figure(figsize=(10, 6))
sns.scatterplot(data=reduced_df, x='x', y='y', hue='êµ°ì§‘ëª…', palette='tab10')
plt.title('ì„œìˆ í˜• ì‘ë‹µ 2D êµ°ì§‘ ì‹œê°í™” (PCA)')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""!pip install streamlit
!pip install pyngrok"""

# Streamlit ì•± ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
"""!nohup streamlit run app.py &"""

"""!ngrok config add-authtoken 2yZkQyd62ZK3i0o12YoBOgE3jtm_4mv46wEnjUtDCM6Gd6zNi

# ngrok í„°ë„ ì—°ê²° ë° ê³µê°œ URL ì¶œë ¥
from pyngrok import ngrok
import time

# Streamlit ì•±ì´ ì‹œì‘ë  ì‹œê°„ì„ ì ì‹œ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
time.sleep(5)

# 8501 í¬íŠ¸ë¡œ ngrok í„°ë„ ì—°ê²°
public_url = ngrok.connect(8501).public_url
print(f"Streamlit app running at: {public_url}")

%%writefile app.py
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm
import os
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
import numpy as np
from sklearn.decomposition import PCA

st.set_page_config(layout="wide") # Use wide layout

st.title("ğŸ“Š ê¸‰ì‹ ì„¤ë¬¸ì¡°ì‚¬ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
st.write("ë³¸ ëŒ€ì‹œë³´ë“œëŠ” ê¸‰ì‹ ì„¤ë¬¸ì¡°ì‚¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.")

# Define the file path to the CSV file
file_path = '/content/drive/MyDrive/á„€á…³á†¸á„‰á…µá†¨ á„‰á…¥á†¯á„†á…®á†«á„Œá…©á„‰á…¡ á„Œá…¥á†«á„á…¦á„€á…µá„€á…¡á†«.csv'

# Load the CSV data into a pandas DataFrame
@st.cache_data
def load_data(path):
    try:
        df = pd.read_csv(path, encoding='utf-8')
        return df
    except FileNotFoundError:
        st.error(f"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
        st.stop() # Stop execution if file not found
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.stop()

df = load_data(file_path)

# Configure matplotlib for Korean font
@st.cache_resource
def init_fonts():
    font_dirs = [os.path.join(os.path.dirname(__file__), 'fonts')]
    font_files = fm.findSystemFonts(fontpaths=font_dirs)
    for font_file in font_files:
        fm.fontManager.addfont(font_file)
    try:
        # Ensure this path is correct for your environment
        font_path_in_colab = '/content/drive/MyDrive/NanumGothic.ttf'
        if os.path.exists(font_path_in_colab):
             fm.fontManager.addfont(font_path_in_colab)
        else:
             st.warning(f"ê¸€ê¼´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path_in_colab}. ê¸°ë³¸ í°íŠ¸ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")

    except Exception as e:
        st.warning(f"ê¸€ê¼´ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") # Use warning if font not found

init_fonts()

plt.rcParams['font.family'] = 'NanumGothic'
plt.rcParams['axes.unicode_minus'] = False # Prevent breaking of '-' in Korean

st.header("ğŸ“„ ì„¤ë¬¸ ì‘ë‹µ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
st.dataframe(df.head())


st.header("ğŸ“Š ê°ê´€ì‹ ë¬¸í•­ ë¶„ì„")
st.write("ì„¤ë¬¸ì¡°ì‚¬ì˜ ê°ê´€ì‹ ë¬¸í•­ì— ëŒ€í•œ ì‘ë‹µ ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.")


# Objective columns for analysis
objective_columns = ['í•™ë…„', 'ì´ë²ˆì£¼ ë§Œì¡±ë„', 'ì´ë²ˆì£¼ ê°€ì¥ ì¢‹ì•˜ë˜ ê¸‰ì‹', 'ì”ë°˜ ë¹„ìœ¨', 'ìˆ˜ë©´ì‹œê°„']
pie_chart_columns = ['ì´ë²ˆì£¼ ë§Œì¡±ë„', 'ì”ë°˜ ë¹„ìœ¨']

# Arrange plots in columns where appropriate
col1, col2 = st.columns(2)

with col1:
    for col in objective_columns[:3]: # First 3 columns in the first column
        if col in df.columns:
            st.subheader(f"{col} ì‘ë‹µ ë¶„í¬")
            st.write("ì‘ë‹µ ìˆ˜:")
            st.dataframe(df[col].value_counts())

            plt.figure(figsize=(10, 6))
            sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette='viridis')
            plt.title(f'{col} ì‘ë‹µ ë¶„í¬')
            plt.xlabel(col)
            plt.ylabel('ì‘ë‹µ ìˆ˜')
            plt.xticks(rotation=45, ha='right') # Rotate labels for better readability
            plt.tight_layout()
            st.pyplot(plt)
            plt.close()

with col2:
    for col in objective_columns[3:]: # Remaining columns in the second column
        if col in df.columns:
            st.subheader(f"{col} ì‘ë‹µ ë¶„í¬")
            st.write("ì‘ë‹µ ìˆ˜:")
            st.dataframe(df[col].value_counts())

            plt.figure(figsize=(10, 6))
            sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette='viridis')
            plt.title(f'{col} ì‘ë‹µ ë¶„í¬')
            plt.xlabel(col)
            plt.ylabel('ì‘ë‹µ ìˆ˜')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            st.pyplot(plt)
            plt.close()

# Pie charts in a single row of columns
st.subheader("ì£¼ìš” ë§Œì¡±ë„ ë° ì”ë°˜ ë¹„ìœ¨")
pie_col1, pie_col2 = st.columns(2)

with pie_col1:
    col = 'ì´ë²ˆì£¼ ë§Œì¡±ë„'
    if col in df.columns:
        st.write(f"**{col} ë¹„ìœ¨**")
        plt.figure(figsize=(8, 8))
        df[col].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, counterclock=False, colors=sns.color_palette('pastel'))
        plt.title(f'{col} ë¹„ìœ¨')
        plt.ylabel('')
        plt.tight_layout()
        st.pyplot(plt)
        plt.close()

with pie_col2:
    col = 'ì”ë°˜ ë¹„ìœ¨'
    if col in df.columns:
        st.write(f"**{col} ë¹„ìœ¨**")
        plt.figure(figsize=(8, 8))
        df[col].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, counterclock=False, colors=sns.color_palette('pastel'))
        plt.title(f'{col} ë¹„ìœ¨')
        plt.ylabel('')
        plt.tight_layout()
        st.pyplot(plt)
        plt.close()


st.header("ğŸ“ ì„œìˆ í˜• ë¬¸í•­ ë¶„ì„ (ê±´ì˜ì‚¬í•­)")
st.write("ì‘ë‹µìë“¤ì´ ì‘ì„±í•œ ê±´ì˜ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ ì£¼ìš” í‚¤ì›Œë“œë³„ë¡œ êµ°ì§‘í™”í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.")

text_column = 'ì¶”ê°€ ë©”ë‰´ì™€ ê±´ì˜ì‚¬í•­'

# Sentence splitting function
def split_sentences(text):
    text = re.sub(r'[.?!]', '', text)
    return re.split(r',|ê·¸ë¦¬ê³ |ë˜ëŠ”|ë°|&|/|ë˜\s+|ê·¸ë¦¬ê³ \s+', text)

@st.cache_data
def process_text_and_cluster(dataframe, text_col, n_clusters=8):
    split_texts = []
    original_indices = []
    original_sentences = []

    for idx, text in dataframe[text_col].dropna().astype(str).items():
        splits = split_sentences(text)
        for part in splits:
            cleaned = part.strip()
            if cleaned:
                split_texts.append(cleaned)
                original_indices.append(idx)
                original_sentences.append(text)

    if not split_texts:
        return None, None, None, None

    # Load the model
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    embeddings = model.encode(split_texts)

    # Perform KMeans clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(embeddings)

    # Create result DataFrame
    result_df = pd.DataFrame({
        'ë¬¸ì¥_ë¶„ì ˆ': split_texts,
        'êµ°ì§‘': labels,
        'ì›ë³¸ë¬¸ì¥_index': original_indices,
        'ì›ë³¸ë¬¸ì¥': original_sentences
    })

    # Determine cluster names using TF-IDF
    cluster_names = {}
    for i in range(n_clusters):
        cluster_data = result_df[result_df['êµ°ì§‘'] == i]
        if not cluster_data.empty:
            cluster_sentences = cluster_data['ë¬¸ì¥_ë¶„ì ˆ'].tolist()
            vectorizer = TfidfVectorizer(max_features=20) # Removed English stop words as corpus is Korean
            tfidf_matrix = vectorizer.fit_transform(cluster_sentences)
            feature_names = vectorizer.get_feature_names_out()
            if feature_names.size > 0:
                scores = tfidf_matrix.sum(axis=0).A1
                top_idx = scores.argmax()
                cluster_keyword = feature_names[top_idx]
            else:
                cluster_keyword = f'êµ°ì§‘ {i}' # Fallback name if no features found
            cluster_names[i] = cluster_keyword
        else:
             cluster_names[i] = f'êµ°ì§‘ {i}' # Fallback name if cluster is empty


    result_df['êµ°ì§‘ëª…'] = result_df['êµ°ì§‘'].map(cluster_names)

    # Perform PCA for visualization
    pca = PCA(n_components=2)
    reduced = pca.fit_transform(embeddings)
    reduced_df = pd.DataFrame({
        'x': reduced[:, 0],
        'y': reduced[:, 1],
        'êµ°ì§‘': result_df['êµ°ì§‘'],
        'êµ°ì§‘ëª…': result_df['êµ°ì§‘ëª…']
    })

    return result_df, reduced_df, cluster_names, embeddings # Return embeddings as well for potential future use


result_df, reduced_df, cluster_names, embeddings = process_text_and_cluster(df, text_column)

if result_df is not None:
    # Cluster count and PCA plot in columns
    cluster_col1, cluster_col2 = st.columns(2)

    with cluster_col1:
        st.subheader("êµ°ì§‘ë³„ ë¬¸ì¥ ìˆ˜")
        cluster_counts = result_df['êµ°ì§‘ëª…'].value_counts().sort_values(ascending=False)
        plt.figure(figsize=(10, 6))
        sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='viridis')
        plt.title('ì„œìˆ í˜• ì‘ë‹µ êµ°ì§‘ë³„ ë¬¸ì¥ ìˆ˜')
        plt.xlabel('êµ°ì§‘ í‚¤ì›Œë“œ')
        plt.ylabel('ë¬¸ì¥ ìˆ˜')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        st.pyplot(plt)
        plt.close()

    with cluster_col2:
        st.subheader("êµ°ì§‘ ì‹œê°í™” (PCA 2D)")
        plt.figure(figsize=(10, 7))
        sns.scatterplot(data=reduced_df, x='x', y='y', hue='êµ°ì§‘ëª…', palette='tab10', legend='full')
        plt.title('ì„œìˆ í˜• ì‘ë‹µ 2D êµ°ì§‘ ì‹œê°í™” (PCA)')
        plt.xlabel('PCA 1')
        plt.ylabel('PCA 2')
        st.legend(title="êµ°ì§‘ëª…", loc='upper left', bbox_to_anchor=(1.05, 1))
        plt.tight_layout()
        st.pyplot(plt)
        plt.close()

    st.subheader("êµ°ì§‘ë³„ ì›ë³¸ ë¬¸ì¥ ëª©ë¡")
    st.write("ì„ íƒí•œ êµ°ì§‘ì— ì†í•˜ëŠ” ì›ë³¸ ê±´ì˜ì‚¬í•­ ëª©ë¡ì…ë‹ˆë‹¤.")
    selected_cluster_name = st.selectbox("êµ°ì§‘ì„ ì„ íƒí•˜ì„¸ìš”:", list(cluster_names.values()))

    if selected_cluster_name:
        selected_cluster_id = [k for k, v in cluster_names.items() if v == selected_cluster_name][0]
        cluster_data = result_df[result_df['êµ°ì§‘'] == selected_cluster_id]
        unique_originals = cluster_data[['ì›ë³¸ë¬¸ì¥']].drop_duplicates().reset_index(drop=True)

        st.write(f"**[êµ°ì§‘ {selected_cluster_id} - \"{selected_cluster_name}\"] ì›ë³¸ ë¬¸ì¥ ëª©ë¡ (ì´ {len(unique_originals)}ê±´):**")
        for j, row in unique_originals.iterrows():
            st.write(f"- {row['ì›ë³¸ë¬¸ì¥']}")

else:
    st.info(f"'{text_column}' ì»¬ëŸ¼ì— ë¶„ì„í•  í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.write("---")
st.write("ëŒ€ì‹œë³´ë“œ ë¶„ì„ ì¢…ë£Œ")

`app.py` íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. Streamlit ëŒ€ì‹œë³´ë“œì— ë°˜ì˜í•˜ë ¤ë©´ ë‹¤ìŒ ì…€ë“¤ì„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”:

1. `!nohup streamlit run app.py &` ì…€
2. `from pyngrok import ngrok...` ì…€
"""
