# -*- coding: utf-8 -*-
"""급식 데이터 분석.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cvVc1qMQ8OBFz5Mw67kuZSLx9wEPqSYi
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer


# SentenceTransformer를 sentence_transformers 라이브러리에서 임포트합니다.
from sentence_transformers import SentenceTransformer
# KMeans를 sklearn.cluster에서 임포트합니다.
from sklearn.cluster import KMeans
# numpy를 np로 임포트합니다.
import numpy as np

import pandas as pd
import os

%%writefile app.py
file_path = '/content/drive/MyDrive/급식 설문조사 전체기간.csv'

df = pd.read_csv(file_path, encoding='utf-8')

# 데이터 확인
print(df.head())

# 객관식
if '학년' in df.columns:
    print(df['학년'].value_counts())
if '이번주 만족도' in df.columns:
    print(df['이번주 만족도'].value_counts())
if '이번주 가장 좋았던 급식' in df.columns:
    print(df['이번주 가장 좋았던 급식'].value_counts())
if '이번주 급식이 좋았던 이유' in df.columns:
    print(df['이번주 급식이 좋았던 이유'].value_counts())
if '이번주 가장 싫었던 급식' in df.columns:
    print(df['이번주 가장 싫었던 급식'].value_counts())
if '잔반 비율' in df.columns:
    print(df['잔반 비율'].value_counts())
if '수면시간' in df.columns:
    print(df['수면시간'].value_counts())

# 객관식 항목 리스트
objective_columns = ['학년', '이번주 만족도', '이번주 가장 좋았던 급식', '잔반 비율', '수면시간']

for col in objective_columns:
    if col in df.columns:
        plt.figure(figsize=(8, 5))
        sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette='Set2')
        plt.title(f'{col} 응답 분포')
        plt.xlabel(col)
        plt.ylabel('응답 수')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

for col in ['이번주 만족도', '잔반 비율']:
    if col in df.columns:
        plt.figure(figsize=(6, 6))
        df[col].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, counterclock=False)
        plt.title(f'{col} 비율')
        plt.ylabel('')
        plt.show()

# 서술형

# 문장 컬럼 지정 (예: '건의사항' 컬럼)
text_column = '추가 메뉴와 건의사항'

# 문장 분할 함수
def split_sentences(text):
    text = re.sub(r'[.?!]', '', text)  # 마침표 등 제거
    return re.split(r',|그리고|또는|및|&|/|또\s+|그리고\s+', text)

# 분절된 문장, 원본문장 인덱스, 원본문장 저장용 리스트
split_texts = []
original_indices = []
original_sentences = []

for idx, text in df[text_column].dropna().astype(str).items():
    splits = split_sentences(text)
    for part in splits:
        cleaned = part.strip()
        if cleaned:
            split_texts.append(cleaned)
            original_indices.append(idx)
            original_sentences.append(text)

# 임베딩
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
embeddings = model.encode(split_texts)

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

"""embeddings = model.encode(texts)"""

sse = []
k_range = range(2, 30)  # 2~14 군집 수 시도
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(embeddings)
    sse.append(kmeans.inertia_)  # SSE 값 저장

plt.plot(k_range, sse, 'bx-')
plt.xlabel('군집 수 (k)')
plt.ylabel('SSE (오차 제곱합)')
plt.title('엘보우 기법으로 최적 군집 수 찾기')
plt.show()

# 군집 수 설정
n_clusters = 8
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
labels = kmeans.fit_predict(embeddings)

# 결과 DataFrame 생성
result_df = pd.DataFrame({
    '문장_분절': split_texts,
    '군집': labels,
    '원본문장_index': original_indices,
    '원본문장': original_sentences
})

# 중복 제거를 위해 set 사용
"""print(f'\n=== [{text_column}] 군집화 결과 (원본 문장 기준) ===')
for i in range(n_clusters):
    cluster_data = result_df[result_df['군집'] == i]
    unique_originals = cluster_data[['원본문장']].drop_duplicates().reset_index(drop=True)

    print(f'\n[군집 {i}] 원본문장 목록 (총 {len(unique_originals)}건):')
    for j, row in unique_originals.iterrows():
        print(f'- {row["원본문장"]}')"""

print(f'\n=== [{text_column}] 군집화 결과 (군집 이름 포함) ===')
cluster_names = {}
for i in range(n_clusters):
    cluster_data = result_df[result_df['군집'] == i]
    cluster_sentences = cluster_data['문장_분절'].tolist()

    # TF-IDF로 중요 단어 추출
    vectorizer = TfidfVectorizer(max_features=20, stop_words='english')  # 필요시 한국어 불용어 처리 가능
    tfidf_matrix = vectorizer.fit_transform(cluster_sentences)
    feature_names = vectorizer.get_feature_names_out()
    scores = tfidf_matrix.sum(axis=0).A1
    top_idx = scores.argmax()
    cluster_keyword = feature_names[top_idx]

    cluster_names[i] = cluster_keyword

    # 원본문장 목록 출력
    unique_originals = cluster_data[['원본문장']].drop_duplicates().reset_index(drop=True)
    print(f'\n[군집 {i} - "{cluster_keyword}"] 원본문장 목록 (총 {len(unique_originals)}건):')
    for j, row in unique_originals.iterrows():
        print(f'- {row["원본문장"]}')

import matplotlib.pyplot as plt
import seaborn as sns

# 군집 키워드 이름 붙이기
result_df['군집명'] = result_df['군집'].map(cluster_names)

# 군집별 개수 세기
cluster_counts = result_df['군집명'].value_counts().sort_values(ascending=False)

plt.figure(figsize=(10, 5))
sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='pastel')
plt.title('서술형 응답 군집별 문장 수')
plt.xlabel('군집 키워드')
plt.ylabel('문장 수')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

from sklearn.decomposition import PCA

# 2D 임베딩으로 축소
pca = PCA(n_components=2)
reduced = pca.fit_transform(embeddings)

reduced_df = pd.DataFrame({
    'x': reduced[:, 0],
    'y': reduced[:, 1],
    '군집': result_df['군집'],
    '군집명': result_df['군집명']
})

plt.figure(figsize=(10, 6))
sns.scatterplot(data=reduced_df, x='x', y='y', hue='군집명', palette='tab10')
plt.title('서술형 응답 2D 군집 시각화 (PCA)')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""!pip install streamlit
!pip install pyngrok"""

# Streamlit 앱 백그라운드 실행
"""!nohup streamlit run app.py &"""

"""!ngrok config add-authtoken 2yZkQyd62ZK3i0o12YoBOgE3jtm_4mv46wEnjUtDCM6Gd6zNi

# ngrok 터널 연결 및 공개 URL 출력
from pyngrok import ngrok
import time

# Streamlit 앱이 시작될 시간을 잠시 기다립니다.
time.sleep(5)

# 8501 포트로 ngrok 터널 연결
public_url = ngrok.connect(8501).public_url
print(f"Streamlit app running at: {public_url}")

%%writefile app.py
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.font_manager as fm
import os
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
import numpy as np
from sklearn.decomposition import PCA

st.set_page_config(layout="wide") # Use wide layout

st.title("📊 급식 설문조사 분석 대시보드")
st.write("본 대시보드는 급식 설문조사 결과를 분석하고 시각화하여 보여줍니다.")

# Define the file path to the CSV file
file_path = '/content/drive/MyDrive/급식 설문조사 전체기간.csv'

# Load the CSV data into a pandas DataFrame
@st.cache_data
def load_data(path):
    try:
        df = pd.read_csv(path, encoding='utf-8')
        return df
    except FileNotFoundError:
        st.error(f"오류: 파일을 찾을 수 없습니다: {path}")
        st.stop() # Stop execution if file not found
    except Exception as e:
        st.error(f"데이터 로딩 중 오류가 발생했습니다: {e}")
        st.stop()

df = load_data(file_path)

# Configure matplotlib for Korean font
@st.cache_resource
def init_fonts():
    font_dirs = [os.path.join(os.path.dirname(__file__), 'fonts')]
    font_files = fm.findSystemFonts(fontpaths=font_dirs)
    for font_file in font_files:
        fm.fontManager.addfont(font_file)
    try:
        # Ensure this path is correct for your environment
        font_path_in_colab = '/content/drive/MyDrive/NanumGothic.ttf'
        if os.path.exists(font_path_in_colab):
             fm.fontManager.addfont(font_path_in_colab)
        else:
             st.warning(f"글꼴 파일을 찾을 수 없습니다: {font_path_in_colab}. 기본 폰트로 표시됩니다.")

    except Exception as e:
        st.warning(f"글꼴 추가 중 오류 발생: {e}") # Use warning if font not found

init_fonts()

plt.rcParams['font.family'] = 'NanumGothic'
plt.rcParams['axes.unicode_minus'] = False # Prevent breaking of '-' in Korean

st.header("📄 설문 응답 데이터 미리보기")
st.dataframe(df.head())


st.header("📊 객관식 문항 분석")
st.write("설문조사의 객관식 문항에 대한 응답 분포를 시각화합니다.")


# Objective columns for analysis
objective_columns = ['학년', '이번주 만족도', '이번주 가장 좋았던 급식', '잔반 비율', '수면시간']
pie_chart_columns = ['이번주 만족도', '잔반 비율']

# Arrange plots in columns where appropriate
col1, col2 = st.columns(2)

with col1:
    for col in objective_columns[:3]: # First 3 columns in the first column
        if col in df.columns:
            st.subheader(f"{col} 응답 분포")
            st.write("응답 수:")
            st.dataframe(df[col].value_counts())

            plt.figure(figsize=(10, 6))
            sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette='viridis')
            plt.title(f'{col} 응답 분포')
            plt.xlabel(col)
            plt.ylabel('응답 수')
            plt.xticks(rotation=45, ha='right') # Rotate labels for better readability
            plt.tight_layout()
            st.pyplot(plt)
            plt.close()

with col2:
    for col in objective_columns[3:]: # Remaining columns in the second column
        if col in df.columns:
            st.subheader(f"{col} 응답 분포")
            st.write("응답 수:")
            st.dataframe(df[col].value_counts())

            plt.figure(figsize=(10, 6))
            sns.countplot(data=df, x=col, order=df[col].value_counts().index, palette='viridis')
            plt.title(f'{col} 응답 분포')
            plt.xlabel(col)
            plt.ylabel('응답 수')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            st.pyplot(plt)
            plt.close()

# Pie charts in a single row of columns
st.subheader("주요 만족도 및 잔반 비율")
pie_col1, pie_col2 = st.columns(2)

with pie_col1:
    col = '이번주 만족도'
    if col in df.columns:
        st.write(f"**{col} 비율**")
        plt.figure(figsize=(8, 8))
        df[col].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, counterclock=False, colors=sns.color_palette('pastel'))
        plt.title(f'{col} 비율')
        plt.ylabel('')
        plt.tight_layout()
        st.pyplot(plt)
        plt.close()

with pie_col2:
    col = '잔반 비율'
    if col in df.columns:
        st.write(f"**{col} 비율**")
        plt.figure(figsize=(8, 8))
        df[col].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, counterclock=False, colors=sns.color_palette('pastel'))
        plt.title(f'{col} 비율')
        plt.ylabel('')
        plt.tight_layout()
        st.pyplot(plt)
        plt.close()


st.header("📝 서술형 문항 분석 (건의사항)")
st.write("응답자들이 작성한 건의사항을 분석하여 주요 키워드별로 군집화하고 시각화합니다.")

text_column = '추가 메뉴와 건의사항'

# Sentence splitting function
def split_sentences(text):
    text = re.sub(r'[.?!]', '', text)
    return re.split(r',|그리고|또는|및|&|/|또\s+|그리고\s+', text)

@st.cache_data
def process_text_and_cluster(dataframe, text_col, n_clusters=8):
    split_texts = []
    original_indices = []
    original_sentences = []

    for idx, text in dataframe[text_col].dropna().astype(str).items():
        splits = split_sentences(text)
        for part in splits:
            cleaned = part.strip()
            if cleaned:
                split_texts.append(cleaned)
                original_indices.append(idx)
                original_sentences.append(text)

    if not split_texts:
        return None, None, None, None

    # Load the model
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    embeddings = model.encode(split_texts)

    # Perform KMeans clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(embeddings)

    # Create result DataFrame
    result_df = pd.DataFrame({
        '문장_분절': split_texts,
        '군집': labels,
        '원본문장_index': original_indices,
        '원본문장': original_sentences
    })

    # Determine cluster names using TF-IDF
    cluster_names = {}
    for i in range(n_clusters):
        cluster_data = result_df[result_df['군집'] == i]
        if not cluster_data.empty:
            cluster_sentences = cluster_data['문장_분절'].tolist()
            vectorizer = TfidfVectorizer(max_features=20) # Removed English stop words as corpus is Korean
            tfidf_matrix = vectorizer.fit_transform(cluster_sentences)
            feature_names = vectorizer.get_feature_names_out()
            if feature_names.size > 0:
                scores = tfidf_matrix.sum(axis=0).A1
                top_idx = scores.argmax()
                cluster_keyword = feature_names[top_idx]
            else:
                cluster_keyword = f'군집 {i}' # Fallback name if no features found
            cluster_names[i] = cluster_keyword
        else:
             cluster_names[i] = f'군집 {i}' # Fallback name if cluster is empty


    result_df['군집명'] = result_df['군집'].map(cluster_names)

    # Perform PCA for visualization
    pca = PCA(n_components=2)
    reduced = pca.fit_transform(embeddings)
    reduced_df = pd.DataFrame({
        'x': reduced[:, 0],
        'y': reduced[:, 1],
        '군집': result_df['군집'],
        '군집명': result_df['군집명']
    })

    return result_df, reduced_df, cluster_names, embeddings # Return embeddings as well for potential future use


result_df, reduced_df, cluster_names, embeddings = process_text_and_cluster(df, text_column)

if result_df is not None:
    # Cluster count and PCA plot in columns
    cluster_col1, cluster_col2 = st.columns(2)

    with cluster_col1:
        st.subheader("군집별 문장 수")
        cluster_counts = result_df['군집명'].value_counts().sort_values(ascending=False)
        plt.figure(figsize=(10, 6))
        sns.barplot(x=cluster_counts.index, y=cluster_counts.values, palette='viridis')
        plt.title('서술형 응답 군집별 문장 수')
        plt.xlabel('군집 키워드')
        plt.ylabel('문장 수')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        st.pyplot(plt)
        plt.close()

    with cluster_col2:
        st.subheader("군집 시각화 (PCA 2D)")
        plt.figure(figsize=(10, 7))
        sns.scatterplot(data=reduced_df, x='x', y='y', hue='군집명', palette='tab10', legend='full')
        plt.title('서술형 응답 2D 군집 시각화 (PCA)')
        plt.xlabel('PCA 1')
        plt.ylabel('PCA 2')
        st.legend(title="군집명", loc='upper left', bbox_to_anchor=(1.05, 1))
        plt.tight_layout()
        st.pyplot(plt)
        plt.close()

    st.subheader("군집별 원본 문장 목록")
    st.write("선택한 군집에 속하는 원본 건의사항 목록입니다.")
    selected_cluster_name = st.selectbox("군집을 선택하세요:", list(cluster_names.values()))

    if selected_cluster_name:
        selected_cluster_id = [k for k, v in cluster_names.items() if v == selected_cluster_name][0]
        cluster_data = result_df[result_df['군집'] == selected_cluster_id]
        unique_originals = cluster_data[['원본문장']].drop_duplicates().reset_index(drop=True)

        st.write(f"**[군집 {selected_cluster_id} - \"{selected_cluster_name}\"] 원본 문장 목록 (총 {len(unique_originals)}건):**")
        for j, row in unique_originals.iterrows():
            st.write(f"- {row['원본문장']}")

else:
    st.info(f"'{text_column}' 컬럼에 분석할 텍스트 데이터가 없습니다.")

st.write("---")
st.write("대시보드 분석 종료")

`app.py` 파일이 업데이트되었습니다. Streamlit 대시보드에 반영하려면 다음 셀들을 다시 실행해주세요:

1. `!nohup streamlit run app.py &` 셀
2. `from pyngrok import ngrok...` 셀
"""
